{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.request import Request, urlopen\n",
    "from selenium import webdriver\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoxScraper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Fox News Web Scraper\"\n",
    "        self.page = 'https://www.foxnews.com/politics'\n",
    "        \n",
    "    def get_urls(self):\n",
    "        self.links=[]\n",
    "        count=0\n",
    "        pg = requests.get(self.page)\n",
    "        if pg.status_code==200:\n",
    "            soup = BeautifulSoup(pg.text,\"xml\")\n",
    "            data = soup.findAll('h4', {'class':\"title\"})\n",
    "        else:\n",
    "            print(\"Failed to get page\")\n",
    "\n",
    "        for i in data:\n",
    "            link= ( i.find('a').get(\"href\"))\n",
    "            if link not in self.links and link.startswith('https://video')==False:\n",
    "                self.links.append('https://www.foxnews.com' + link)\n",
    "        #for link in links:\n",
    "            #urls.insert_one({'link': link})\n",
    "        return self.links\n",
    "    \n",
    "    def get_info(self):\n",
    "        client = MongoClient()\n",
    "        self.fox_db = client['fox']\n",
    "        self.urls = self.fox_db['urls']\n",
    "        self.articles = self.fox_db['articles']\n",
    "        for link in self.links:\n",
    "            pg = requests.get(link)\n",
    "            if pg.status_code == 200:\n",
    "                soup = BeautifulSoup(pg.text,\"xml\")\n",
    "                data = soup.findAll('div', {'class':\"article-body\"})\n",
    "                content = []\n",
    "                for i in data:\n",
    "                    for j in i.find_all('p'):\n",
    "                        t = j.get_text()\n",
    "                        content.append(t)\n",
    "                content = \" \".join(content)\n",
    "                title = soup.find('h1').text\n",
    "                key = {\"title\":title}\n",
    "                value = {'title':title, 'content': content}\n",
    "                (self.articles).update(key,value,upsert=True)\n",
    "                time.sleep(2)\n",
    "                \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersScraper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Reuters Web Scraper\"\n",
    "        self.page = 'https://www.reuters.com/politics'\n",
    "        \n",
    "    def get_urls(self):\n",
    "        self.links=[]\n",
    "        count=0\n",
    "        pg = requests.get(self.page)\n",
    "        \n",
    "        req = Request(self.page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage,\"lxml\")\n",
    "        data = soup.findAll('h2', {'class':\"FeedItemHeadline_headline\"})\n",
    "        \n",
    "\n",
    "        for i in data:\n",
    "            link= ( i.find('a').get(\"href\"))\n",
    "            if link not in self.links:\n",
    "                self.links.append(link)\n",
    "        return self.links\n",
    "    \n",
    "    def get_info(self):\n",
    "        client = MongoClient()\n",
    "        self.reuters_db = client['reuters']\n",
    "        self.urls = self.reuters_db['urls']\n",
    "        self.articles = self.reuters_db['articles']\n",
    "        for link in self.links:\n",
    "            req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            webpage = urlopen(req).read()\n",
    "            soup = BeautifulSoup(req,'html.parser')\n",
    "            data = soup.findAll('div', {'class':\"StandardArticleBody_body\"})\n",
    "            content = []\n",
    "            for i in data:\n",
    "                for j in i.find_all('p'):\n",
    "                    t = j.get_text()\n",
    "                    content.append(t)\n",
    "            content = \" \".join(content)\n",
    "            title = soup.find('h1', {\"class\" : \"ArticleHeader_headline\"}).text\n",
    "            key = {\"title\":title}\n",
    "            value = {'title':title, 'content': content}\n",
    "            (self.articles).update(key,value,upsert=True)\n",
    "            time.sleep(2)\n",
    "            \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffScraper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"CNN Web Scraper\"\n",
    "        self.page = 'https://www.huffpost.com/news/politics'\n",
    "        \n",
    "    def get_urls(self):\n",
    "        self.links=[]\n",
    "        count=0\n",
    "        req = Request(self.page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "        data = soup.findAll('div', {'class':\"card__headlines\"})\n",
    "        for i in data:\n",
    "            link= ( i.find('a').get(\"href\"))\n",
    "            if link not in self.links:\n",
    "                self.links.append(link)\n",
    "        return self.links\n",
    "    \n",
    "    def get_info(self):\n",
    "        client = MongoClient()\n",
    "        self.huffpost_db = client['huffpost']\n",
    "        self.urls = self.huffpost_db['urls']\n",
    "        self.articles = self.huffpost_db['articles']\n",
    "        for link in self.links[1:]:\n",
    "            driver = webdriver.Chrome(executable_path='./src/chromedriver')\n",
    "            driver.get(link)\n",
    "            soup = BeautifulSoup(driver.page_source,\"lxml\")\n",
    "            data = soup.findAll('div', {'id':\"entry-text\"})\n",
    "            content = []\n",
    "            for i in data:\n",
    "                for j in i.find_all('p'):\n",
    "                    t = j.get_text()\n",
    "                    content.append(t)\n",
    "            content = \" \".join(content)\n",
    "            if soup.find('h1', {\"class\" : \"headline__title\"}) != None:\n",
    "                title = soup.find('h1', {\"class\" : \"headline__title\"}).text\n",
    "            key = {\"title\":title}\n",
    "            value = {'title':title, 'content': content}\n",
    "            (self.articles).update(key,value,upsert=True)\n",
    "            time.sleep(2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
