{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = np.arange(1,401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "breitbart_db=client['breitbart']\n",
    "urls = breitbart_db['urls']\n",
    "articles = breitbart_db['articles']\n",
    "\n",
    "\n",
    "def get_urls(x):\n",
    "    links=[]\n",
    "    links2=[]\n",
    "    count=0\n",
    "    first_half = \"https://www.breitbart.com\"\n",
    "    for num in x:\n",
    "        pg = requests.get(f'https://www.breitbart.com/politics/page/{num}/')\n",
    "        if pg.status_code==200:\n",
    "            soup = BeautifulSoup(pg.text,\"lxml\")\n",
    "            data = soup.findAll('div', {'class':\"tC\"})\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        for i in data:\n",
    "            link=first_half+(i.find('a').get(\"href\"))\n",
    "            if link not in links:\n",
    "                links.append(link)          \n",
    "        print(f'Scraped {len(links)} urls.')\n",
    "        time.sleep(2)\n",
    "    for link in links:\n",
    "        urls.insert_one({'link': link})\n",
    "    print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(urls_of_articles):\n",
    "    count=0\n",
    "    for url in urls_of_articles:\n",
    "        pg = requests.get(url)\n",
    "        if pg.status_code==200:\n",
    "            soup = BeautifulSoup(pg.text,\"lxml\")\n",
    "            title = soup.find('h1').text\n",
    "            author = soup.find('div', {\"class\": 'header_byline'}).find('a').text\n",
    "            date = soup.find('div', {\"class\": 'header_byline'}).find('time').text\n",
    "            shares = soup.find('span', {\"class\": 'acz5'}).text\n",
    "            \n",
    "            data = soup.findAll('div', {'class':\"entry-content\"})\n",
    "            content=[]\n",
    "            for i in data:\n",
    "                for j in i.find_all('p'):\n",
    "                    t = j.get_text()\n",
    "                    content.append(t)\n",
    "                content = \" \".join(content)\n",
    "            print(title,author,date)\n",
    "            articles.insert_one({'title':title, 'author':author, 'date': date, 'content': content})\n",
    "            count+=1\n",
    "            print(f'Scraped {count} articles.')\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            print('Failed to get page!')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.breitbart.com/europe/2020/03/04/tory-mp-civil-servants-said-britons-got-it-wrong-when-they-voted-brexit/'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# websites = [urls.find()[i]['link'] for i in range(5001)\n",
    "# get_articles(websites)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
