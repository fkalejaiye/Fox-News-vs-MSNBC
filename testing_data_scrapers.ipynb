{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnScraper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"CNN Web Scraper\"\n",
    "        self.page = 'https://www.cnn.com/politics'\n",
    "        \n",
    "    def initialize_database(self,website):\n",
    "        client = MongoClient()\n",
    "        self.cnn_db = client['breitbart']\n",
    "        self.urls = self.cnn_db['urls']\n",
    "        self.articles = self.cnn_db['articles']\n",
    "        \n",
    "    def get_urls(self):\n",
    "        links=[]\n",
    "        count=0\n",
    "        pg = requests.get(self.page)\n",
    "        if pg.status_code==200:\n",
    "            soup = BeautifulSoup(pg.text,\"lxml\")\n",
    "            data = soup.findAll('dic', {'class':\"cd__status\"})\n",
    "        else:\n",
    "            print(\"Failed to get page\")\n",
    "\n",
    "        for i in data:\n",
    "            link= self.page +( i.find('a').get(\"href\"))\n",
    "            if link not in links:\n",
    "                links.append(link)          \n",
    "            print(f'Scraped {len(links)} urls.')\n",
    "            time.sleep(2)\n",
    "        #for link in links:\n",
    "            #urls.insert_one({'link': link})\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CnnScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CnnScraper.initialize_database of <__main__.CnnScraper object at 0x10e851e10>>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.initialize_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = c.get_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.find_all('div', {'class':\"media\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(x):\n",
    "    links=[]\n",
    "    count=0\n",
    "    first_half = \"https://www.breitbart.com\"\n",
    "    for num in x:\n",
    "        pg = requests.get(f'https://www.breitbart.com/politics/page/{num}/')\n",
    "        if pg.status_code==200:\n",
    "            soup = BeautifulSoup(pg.text,\"lxml\")\n",
    "            data = soup.findAll('div', {'class':\"tC\"})\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        for i in data:\n",
    "            link=first_half+(i.find('a').get(\"href\"))\n",
    "            if link not in links:\n",
    "                links.append(link)          \n",
    "        print(f'Scraped {len(links)} urls.')\n",
    "        time.sleep(2)\n",
    "    for link in links:\n",
    "        urls.insert_one({'link': link})\n",
    "    print(\"Done!!\")\n",
    "\n",
    "\n",
    "def get_articles(urls_of_articles):\n",
    "    count=0\n",
    "    for url in urls_of_articles:\n",
    "        pg = requests.get(url)\n",
    "        if pg.status_code==200:\n",
    "            soup = BeautifulSoup(pg.text,\"lxml\")\n",
    "            title = soup.find('h1').text\n",
    "            author = soup.find('div', {\"class\": 'header_byline'}).find('a').text\n",
    "            date = soup.find('div', {\"class\": 'header_byline'}).find('time').text\n",
    "            shares = soup.find('span', {\"class\": 'acz5'}).text\n",
    "            \n",
    "            data = soup.findAll('div', {'class':\"entry-content\"})\n",
    "            content=[]\n",
    "            for i in data:\n",
    "                for j in i.find_all('p'):\n",
    "                    t = j.get_text()\n",
    "                    content.append(t)\n",
    "            content = \" \".join(content)\n",
    "            key = {\"title\":title}\n",
    "            value = {'title':title, 'author':author, 'date': date, 'content': content}\n",
    "            articles.update(key,value,upsert=True)\n",
    "            count+=1\n",
    "            print(f'Scraped {count} articles. ({title})')\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            print('Failed to get page!')\n",
    "            continue\n",
    "\n",
    "websites = [urls.find()[i]['link'] for i in range(5000,7500)]\n",
    "get_articles(websites)\n",
    "print('Finished Scraping!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
